{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyPI project classification based on package description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an experimental notebook with the goal of exploring different methods to extend [project2vec](https://developers.redhat.com/articles/2021/10/06/find-and-compare-python-libraries-project2vec).\n",
    "\n",
    "The data used for this project consists in information about ~70 000 Python packages aggregated from PyPI using [Selinon](https://github.com/thoth-station/selinon-worker) and is available under ``s3://DH-DEV-DATA/data/thoth/selinon/pypi_project/ProjectInfo/`` on Ceph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each file describes a different package hosted on PyPI, providing metadata about the project in the `info` field. We are interested in the following subfields:\n",
    "- `classifiers`\n",
    "- `keywords`\n",
    "- `description`\n",
    "\n",
    "The classifiers of a project consist in a hierchical labels, with their [official list](https://pypi.org/classifiers/) described on the PyPI website. They allow to classify a project according to different categories and scopes, for example by language and version, operating system or topic. In the case of the `selinon` package, the classifiers are the following:\n",
    "\n",
    "```\n",
    "      \"Development Status :: 4 - Beta\",\n",
    "      \"Intended Audience :: Developers\",\n",
    "      \"License :: OSI Approved :: BSD License\",\n",
    "      \"Operating System :: OS Independent\",\n",
    "      \"Programming Language :: Python :: 3\",\n",
    "      \"Programming Language :: Python :: 3.4\",\n",
    "      \"Programming Language :: Python :: 3.5\",\n",
    "      \"Programming Language :: Python :: 3.6\",\n",
    "      \"Programming Language :: Python :: Implementation :: CPython\",\n",
    "      \"Programming Language :: Python :: Implementation :: PyPy\",\n",
    "      \"Topic :: System :: Distributed Computing\"\n",
    "```\n",
    "\n",
    "The keywords consist in a few words written in free text format (often separated by a whitespace or a coma) describing the package in a non-hierchical way. The keywords chosen to describe `selinon` are:\n",
    "\n",
    "``selinon celery yaml flow distributed-computing``\n",
    "\n",
    "Finally, the description is the text describing the project as seen in the PyPI pakage page.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of a raw content dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create datasets of descriptions from those PyPI projects and link each project and its description respectively to its trove classifiers and keywords from PyPI.\n",
    "The dataset should contain the following information:\n",
    "\n",
    "``Project name | Project description | Project PyPI classifiers or Project PyPI keywords``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import csv\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import PrettyPrinter\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "from typing import Dict\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "_DATA_PATH = \"data/\"\n",
    "_DATASETS_SAVING_PATH = \"datasets/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling and saving the datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatting the projects descriptions by removing punctuation is necessary for text preprocessing purposes and to avoid any conflicts when storing data in CSV format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_description(description: str, threshold: int) -> str:\n",
    "    \"\"\"Normalize the description and remove special characters, non-English characters and words longer than a threshold.\"\"\"\n",
    "    symbols = \"!\\\"#$%&()*+-/:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for symbol in symbols:\n",
    "        description = description.replace(symbol, \" \")\n",
    "    description = \" \".join([word.strip().lower() for word in description.split(\" \") if word != \"\"])\n",
    "\n",
    "    words_list = description.split(\" \")\n",
    "    for word in words_list:\n",
    "        if len(word) >= threshold:\n",
    "            words_list.remove(word)\n",
    "    formatted_description = \" \".join(words_list).encode(\"ascii\", \"ignore\").decode()\n",
    "\n",
    "    return formatted_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_keywords(keywords: str, threshold: int) -> str:\n",
    "    \"\"\"Format free text PyPI keywords of the projects.\"\"\"\n",
    "    keywords = str(keywords)\n",
    "    if \",\" in keywords:\n",
    "        return format_description(keywords.replace(\",\", \" \"), threshold)\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we need only projects containing a full description, we filter those with an empty or unknown one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def description_is_valid(description: str) -> bool:\n",
    "    \"\"\"Verify if a description is present and valid.\"\"\"\n",
    "    if description in [None, \"\", \" \"] or description.startswith((\"Unknown\", \"unknown\", \"UNKNOWN\", \"\\n\")):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projects_with_descriptors_dataset(descriptors: str, words_length_threshold: int) -> Dict[str, list]:\n",
    "    \"\"\"Retrieve projects with PyPI classifiers or keywords from data aggregated with Selinon.\"\"\"\n",
    "    projects_with_descriptors_dataset = {}\n",
    "    for root, dirs, files in os.walk(os.path.join(_DATA_PATH)):\n",
    "        for file in files:\n",
    "            with open(os.path.join(_DATA_PATH, file), \"r\") as json_file:\n",
    "                file_content = json.loads(json_file.read())\n",
    "            \n",
    "                if descriptors == \"classifiers\":\n",
    "                    if description_is_valid(file_content[\"info\"][\"description\"]) and file_content[\"info\"][\"classifiers\"] != []:\n",
    "                        project_description = format_description(file_content[\"info\"][\"description\"], words_length_threshold)\n",
    "                        projects_with_descriptors_dataset[file] = [project_description, file_content[\"info\"][\"classifiers\"]]\n",
    "                elif descriptors == \"keywords\":\n",
    "                    if description_is_valid(file_content[\"info\"][\"description\"]) and file_content[\"info\"][\"keywords\"] not in [\"\", None]:\n",
    "                        project_description = format_description(file_content[\"info\"][\"description\"], words_length_threshold)\n",
    "                        projects_with_descriptors_dataset[file] = [project_description, format_keywords(file_content[\"info\"][\"keywords\"], words_length_threshold)]\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid descriptors name specified\")\n",
    "\n",
    "    return projects_with_descriptors_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test the project, we create a subsample of the datasets by selecting randomly a limited number of packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_PROJECTS = 100\n",
    "WORD_LENGTH_THRESHOLD = 25\n",
    "\n",
    "projects_with_keywords_dataset = get_projects_with_descriptors_dataset(\"keywords\", WORD_LENGTH_THRESHOLD)\n",
    "projects_with_classifiers_dataset = get_projects_with_descriptors_dataset(\"classifiers\", WORD_LENGTH_THRESHOLD)\n",
    "\n",
    "random_keys = list(set(list(projects_with_keywords_dataset.keys())).intersection(set(list(projects_with_classifiers_dataset.keys()))))\n",
    "\n",
    "random.shuffle(random_keys)\n",
    "random_keys = random_keys[:min(len(random_keys)-1, NUMBER_OF_PROJECTS)]\n",
    "\n",
    "with open(os.path.join(_DATASETS_SAVING_PATH, f\"projects_with_keywords_dataset_{NUMBER_OF_PROJECTS}.csv\"), \"w\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for project_name in random_keys:\n",
    "        writer.writerow([project_name, projects_with_keywords_dataset[project_name][0], projects_with_keywords_dataset[project_name][1]])\n",
    "\n",
    "with open(os.path.join(_DATASETS_SAVING_PATH, f\"projects_with_classifiers_dataset_{NUMBER_OF_PROJECTS}.csv\"), \"w\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for project_name in random_keys:\n",
    "        writer.writerow([project_name, projects_with_classifiers_dataset[project_name][0], \",\".join(projects_with_classifiers_dataset[project_name][1])])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now built two datasets containing each project's name and description, followed by corresponding classifiers or keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token ranking using TF-IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step consists in building a dataset that contains tokens of the description for each package ranked by importance based on the [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) tokens ranking method. \n",
    "\n",
    "TF-IDF (for Term Frequency - Inverse Document Frequency) is a statistical method to find the most important terms of a document from a document corpus, i.e. a set of documents. The technique relies on an analysis of the frequency of terms in tokenized and pre-processed documents, where the most frequent terms across documents of the corpus are penalized to highlight the terms appearing more frequently in the document, which allows to identify the topic of the analyzed text. The corpus of documents considered in this analysis is the set of text descriptions from PyPI Python packages which are described with at least one keyword on the PyPI index. \n",
    "\n",
    "A ranking of tokens with TF-IDF can be used to identify the topic of a package which has not been classified yet by computing a similarity measure between its vectorized version with TF-IDF and specific keywords. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of the descriptions text preprocessing is to filter them in order to extract only relevant words (removing stopwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mcostant/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_dataset(dataset_name: str) -> Dict[str, list]:\n",
    "    \"\"\"Remove stop words and put tokens in lowercase.\"\"\"\n",
    "    preprocessed_dataset = {}\n",
    "    stopwords_complete_list = stopwords.words('english') + [stopword.capitalize() for stopword in stopwords.words('english')]\n",
    "\n",
    "    with open(os.path.join(_DATASETS_SAVING_PATH, dataset_name), \"r\") as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "        for row in reader:\n",
    "            preprocessed_description = list(filter(str.isalnum, row[1].split(\" \")))\n",
    "            preprocessed_description = [word.lower() for word in preprocessed_description if word not in stopwords_complete_list]\n",
    "            preprocessed_dataset[row[0]] = [preprocessed_description, row[2].split(\",\")]\n",
    "\n",
    "    return preprocessed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9223372036854775807"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Increasing field size limit to avoid size-related errors:\n",
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a dataset containing only the project names and descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_projects_with_classifiers_dataset = preprocess_dataset(f\"projects_with_classifiers_dataset_{NUMBER_OF_PROJECTS}.csv\")\n",
    "\n",
    "# Print the dataset:\n",
    "\n",
    "# pp = PrettyPrinter(indent=2)\n",
    "# pp.pprint(preprocessed_projects_with_classifiers_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_projects_with_keywords_dataset = preprocess_dataset(f\"projects_with_keywords_dataset_{NUMBER_OF_PROJECTS}.csv\")\n",
    "\n",
    "# Print the dataset:\n",
    "\n",
    "# pp = PrettyPrinter(indent=2)\n",
    "# pp.pprint(preprocessed_projects_with_keywords_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_projects_with_keywords_dataset_ordered = OrderedDict(sorted(preprocessed_projects_with_keywords_dataset.items()))\n",
    "\n",
    "corpus_keywords = [\" \".join(description[0]) for package_name, description in preprocessed_projects_with_keywords_dataset_ordered.items()]\n",
    "\n",
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "tf_idf_keywords = vectorizer.fit_transform(corpus_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a first overview of the results we get from the TF-IDF token ranking by comparing the last obtained rankings for the first 10 packages to the corresponding project descriptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package name:  adafruit-circuitpython-amg88xx\n",
      "Description:  introduction .. image https readthedocs.org projects adafruit circuitpython amg88xx badge version latest target https projects amg88xx en latest alt documentation status .. image https img.shields.io discord 327254708534116352.svg target https discord.gg nbqh6qu alt discord .. image https travis ci.com adafruit adafruit circuitpython amg88xx.svg branch master target https travis ci.com adafruit adafruit circuitpython amg88xx alt build status adafruit circuitpython module for the amg88xx grid eye ir 8x8 thermal camera. dependencies this driver depends on adafruit circuitpython https github.com adafruit circuitpython bus device https github.com adafruit adafruit circuitpython busdevice register https github.com adafruit adafruit circuitpython register please ensure all dependencies are available on the circuitpython filesystem. this is easily achieved by downloading the adafruit library and driver bundle https github.com adafruit adafruit circuitpython bundle . usage example of course, you must import the library to use it .. code block python import busio import adafruit amg88xx the way to create an i2c object depends on the board you are using. for boards with labeled scl and sda pins, you can .. code block python import board you can also use pins defined by the onboard microcontroller through the microcontroller.pin module. now, to initialize the i2c bus .. code block python i2c bus busio.i2c board.scl, board.sda once you have created the i2c interface object, you can use it to instantiate the amg88xx object .. code block python amg adafruit amg88xx.amg88xx i2c bus you can also optionally use the alternate i2c address make sure to solder the jumper on the back of the board if you want to do this .. code block python amg adafruit amg88xx.amg88xx i2c bus, addr 0x68 pixels can be then be read by doing .. code block python print amg.pixels contributing contributions are welcome please read our code of conduct https github.com adafruit adafruit circuitpython lis3dh blob master code of conduct.md before contributing to help this project stay welcoming. building locally to build this library locally you'll need to install the circuitpython travis build tools https github.com adafruit circuitpython build tools package. .. code block shell python3 m venv .env source .env bin activate pip install r requirements.txt once installed, make sure you are in the virtual environment .. code block shell source .env bin activate then run the build .. code block shell circuitpython build bundles filename prefix adafruit circuitpython lis3dh library location .\n",
      "Highest ranked keywords:\n",
      "\n",
      "                 TF-IDF\n",
      "adafruit       0.688682\n",
      "circuitpython  0.469556\n",
      "i2c            0.219126\n",
      "amg88xx        0.187822\n",
      "block          0.179173\n",
      "code           0.146129\n",
      "bus            0.125215\n",
      "https          0.116065\n",
      "build          0.099956\n",
      "board          0.093911\n",
      "shell          0.076428\n",
      "discord        0.062607\n",
      "bundle         0.062607\n",
      "locally        0.062607\n",
      "lis3dh         0.062607\n",
      "amg            0.062607\n",
      "library        0.061999\n",
      "alt            0.058197\n",
      "depends        0.053791\n",
      "contributing   0.053791\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Package name:  alp-util\n",
      "Description:  alp util under development... how to install pip3 install alp util\n",
      "Highest ranked keywords:\n",
      "\n",
      "             TF-IDF\n",
      "alp        0.688003\n",
      "util       0.631326\n",
      "pip3       0.267218\n",
      "install    0.238062\n",
      "plain      0.000000\n",
      "planned    0.000000\n",
      "planning   0.000000\n",
      "platform   0.000000\n",
      "platforms  0.000000\n",
      "pliki      0.000000\n",
      "play       0.000000\n",
      "playing    0.000000\n",
      "plays      0.000000\n",
      "placing    0.000000\n",
      "plays3     0.000000\n",
      "please     0.000000\n",
      "plenty     0.000000\n",
      "00         0.000000\n",
      "pliku      0.000000\n",
      "plikw      0.000000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Package name:  anncolvar\n",
      "Description:  pypi anaconda buildstatus codecov anncolvar collective variables by artificial neural networks usage anncolvar h i infile p intop c colvar col col boxx boxx boxy boxy boxz boxz nofit nofit testset testset shuffle shuffle layers layers layer1 layer1 layer2 layer2 layer3 layer3 actfun1 actfun1 actfun2 actfun2 actfun3 actfun3 optim optim loss loss epochs epochs batch batch o ofile model modelfile plumed plumedfile artificial neural network learning of collective variables of molecular systems, requires numpy, keras and mdtraj optional arguments h, help show this help message and exit i infile input trajectory in pdb, xtc, trr, dcd, netcdf or mdcrd, warning the trajectory must be 1. must contain only atoms to be analyzed, 2. must not contain any periodic boundary condition issues p intop input topology in pdb, warning the structure must be 1. centered in the pbc box and 2. must contain only atoms to be analyzed c colvar input collective variable file in text format, must contain the same number of lines as frames in the trajectory col col the index of the column containing collective variables in the input collective variable file boxx boxx size of x coordinate of pbc box from 0 to set value in nm boxy boxy size of y coordinate of pbc box from 0 to set value in nm boxz boxz size of z coordinate of pbc box from 0 to set value in nm nofit nofit disable fitting, the trajectory must be properly fited default false testset testset size of test set fraction of the trajectory, default 0.1 shuffle shuffle shuffle trajectory frames to obtain training and test set default true layers layers number of hidden layers allowed values 1 3, default 1 layer1 layer1 number of neurons in the first encoding layer default 256 layer2 layer2 number of neurons in the second encoding layer default 256 layer3 layer3 number of neurons in the third encoding layer default 256 actfun1 actfun1 activation function of the first layer default sigmoid, for options see keras documentation actfun2 actfun2 activation function of the second layer default linear, for options see keras documentation actfun3 actfun3 activation function of the third layer default linear, for options see keras documentation optim optim optimizer default adam, for options see keras documentation loss loss loss function default mean squared error, for options see keras documentation epochs epochs number of epochs default 100, 1000 may be necessary for real life applications batch batch batch size 0 no batches, default 256 o ofile output file with original and approximated collective variables txt, default no output model modelfile prefix for output model files experimental, default no output plumed plumedfile output file for plumed default plumed.dat introduction biased simulations, such as metadynamics, use a predefined set of parameters known as collective variables. an artificial bias force is applied on collective variables to enhance sampling. there are two conditions for a parameter to be applied as a collective variable. first, the value of the collective variables can be calculated solely from atomic coordinates. second, the force acting on collective variables can be converted to the force acting on individual atoms. in the other words, it is possible to calculate the first derivative of the collective variables with respect to atomic coordinates. both calculations must be fast enough, because they must be evaluated in every step of the simulation. there are many potential collective variables that cannot be easily calculated. it is possible to calculate the collective variable for hundreds or thousands of structures, but not for millions of structures which is necessary for nanosecond long simulations . anncolvar can approximate such collective variables using a neural network. installation you have to chose and install one of keras backends, such as tensorflow, theano or cntk. for this follow one of these links tensorflow theano cntk next, install anncolvar by pip pip install anncolvar if you use anaconda type conda install c spiwokv anncolvar usage a series of representative structures hundreds or more with pre calculated values of the collective variable is used to train the neural network. the user can specify the input set of reference structures i in the form of a trajectory in pdb, xtc, trr, dcd, netcdf or mdcrd. the trajectory must contain only atoms to be analyzed for example only non hydrogen atoms . the trajectory must not contain any periodic boundary condition issues. both conversions can be made by molecular dynamics simulation packages, for example by gmx trjconv . it is not necessary to fit frames to a reference structure. it is possible to switch fitting off by nofit true . it is necessary to supply an input topology in pdb. this is a structure used as a template for fitting. it is also used to define a box. this box must be large enough to fit the molecule in all frames of the trajectory. it should not be too large because this suppresses non linearity in the neural network. when the user decides to use a 3x3x3 nm box it is necessary to place the molecule to be centered at coordinates 1.5,1.5,1.5 nm. in gromacs it is possible to use gmx editconf f mol.pdb o reference.pdb c box 3 3 3 it must also contain only atoms to be analyzed. size of the box can be specified by parameters boxx , boxy and boxz in nm . last input file is the collective variable file. it is a space separated text file with the same number of lines as the number of frames in the input trajectory. the index of the column can be specified by col e.g. col 2 for the second column of the file. the option testset can control the fraction of the trajectory used as the test set. for example testset 0.1 means that 10 of input data is used as the test set and 90 as the training set. the option shuffle true causes that first 90 is used as the training set and remaining 10 as the test set. otherwise frames are shuffled before separation to the training and test set. the architecture of the neural network is controlled by multiple parameters. the input layer contains 3n neurons where n is the number of atoms . the number of hidden layers is controlled by layers . this can be 1, 2 or 3. for higher number of layers contact the authors. number of neurons in the first, second and third layer is controlled by layer1 , layer2 and layer3 . it is useful to use the number of layers equal to powers of 2 32, 64, 128 etc. . huge numbers of neurons can cause that the program is slow or run out of memory. activation functions of neurons can be controlled by actfun1 , actfun2 and actfun3 . any activation function supported by keras can be used. the optimizer used in the training process can be controlled by optim . the default adam optimizer optim adam works well. the loss function can be controlled by loss . the default loss mean squared error works well. the number of epochs can be controlled by epochs . the default value 100 is quite little, usually 1000 is necessary for real life applications. the batch size can be controlled by batch batch 0 for no batches, default is 256 . output is written into the text file o . it contains the approximated and the original values of collective variable. the model can be stored in the set of text files try model . the input file is printed into the file controlled by plumed by default plumed.dat . this file can be directly used to calculate the evolution of the collective variable by plumed driver or by plumed patched molecular dynamics engine. to use the collective variable in enhances sampling for example metadynamics it is necessary to add a suitable keyword for example metad . .. pypi image https img.shields.io pypi v anncolvar.svg target https pypi.org project anncolvar alt latest version released on pypi .. anaconda image https anaconda.org spiwokv anncolvar badges version.svg target https anaconda.org spiwokv anncolvar alt latest version released on anaconda cloud .. buildstatus image https travis ci.org spiwokv anncolvar.svg branch master target http travis ci.org spiwokv anncolvar alt build status of the master branch on mac linux at travis ci .. codecov image https codecov.io gh spiwokv anncolvar branch master graph badge.svg target https codecov.io gh spiwokv anncolvar alt code coverage .. tensorflow https www.tensorflow.org install .. theano http deeplearning.net software theano install.html .. cntk https docs.microsoft.com en us cognitive toolkit setup cntk on your machine\n",
      "Highest ranked keywords:\n",
      "\n",
      "              TF-IDF\n",
      "collective  0.343444\n",
      "anncolvar   0.224566\n",
      "default     0.219102\n",
      "layers      0.168424\n",
      "trajectory  0.168424\n",
      "controlled  0.154550\n",
      "variables   0.152300\n",
      "number      0.151306\n",
      "keras       0.149711\n",
      "input       0.142947\n",
      "layer       0.137378\n",
      "batch       0.137378\n",
      "must        0.134677\n",
      "neurons     0.130997\n",
      "spiwokv     0.130997\n",
      "loss        0.128627\n",
      "box         0.121840\n",
      "epochs      0.120205\n",
      "testset     0.112283\n",
      "frames      0.112283\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Package name:  ar-markers\n",
      "Description:  .. image https walchko ar markers master pics marker.png  target https github.com walchko ar markers ar markers .. image https img.shields.io pypi l ar markers.svg  target https github.com walchko ar markers .. image https img.shields.io pypi pyversions ar markers.svg  target https github.com walchko ar markers .. image https img.shields.io pypi wheel ar markers.svg  target https github.com walchko ar markers .. image https img.shields.io pypi v ar markers.svg  target https github.com walchko ar markers detection of hamming markers for opencv written in python. originally written by max brauer github https github.com debvortex python ar markers . all i did was clean up some little stuff and package it for use on pypi. this package is able to read and create hamming markers, described in this blogpost http iplimage.com blog approach encodedecode black white marker . purpose this project was for a robotics computer vision class https github.com marsuniversity ece387 i taught spring 2018. i wanted something simple enough we could go through the code and they could understood how it worked. i also taught them opencv, so i wanted something written in that. eventually we made street signs and the students drove roomba robots around on these strees ok, really it was 3 inch wide black tape for the roads . when they detected an intersection, they used a camera to read the street sign ar marker and it told them to go straight, turn left, or turn right. sometimes it isn't as robust as i would like, so you may have to move the target around before it gets recognized. install the simplest way to install is pip install ar markers you will also need opencv 3.x as a minimum. on macos you can do brew install opencv usage there are two helper scripts ar markers generate.py to generate the markers. do ar markers generate.py help to see the options ar markers scan.py to scan the marker. once you have created and printed out a marker, hold the marker in front of your camera. you will see a blue border around the marker, if detected and a green number, showing the id the marker represents. or use in a program like .. code block python  usr bin env python from future import print function import cv2 from ar markers import detect markers if name ' main ' print 'press q to quit' capture cv2.videocapture 0 if capture.isopened try to get the first frame frame captured, frame capture.read else frame captured false while frame captured markers detect markers frame for marker in markers marker.highlite marker frame cv2.imshow 'test frame', frame if cv2.waitkey 1 0xff ord 'q' break frame captured, frame capture.read  when everything done, release the capture capture.release cv2.destroyallwindows bsd license copyright c 2007, max brauer all rights reserved. redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met 1. redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and or other materials provided with the distribution. 3. neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. this software is provided by the copyright holders and contributors as is and any express or implied warranties, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose are disclaimed. in no event shall the copyright holder or contributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages including, but not limited to, procurement of substitute goods or services loss of use, data, or profits or business interruption however caused and on any theory of liability, whether in contract, strict liability, or tort including negligence or otherwise arising in any way out of the use of this software, even if advised of the possibility of such damage.\n",
      "Highest ranked keywords:\n",
      "\n",
      "                   TF-IDF\n",
      "ar               0.540214\n",
      "markers          0.540214\n",
      "frame            0.300119\n",
      "walchko          0.180071\n",
      "marker           0.180071\n",
      "copyright        0.134239\n",
      "https            0.111276\n",
      "opencv           0.090036\n",
      "written          0.089493\n",
      "target           0.084851\n",
      "contributors     0.082619\n",
      "conditions       0.077356\n",
      "around           0.073274\n",
      "image            0.064592\n",
      "hamming          0.060024\n",
      "black            0.060024\n",
      "brauer           0.060024\n",
      "taught           0.060024\n",
      "holder           0.060024\n",
      "redistributions  0.060024\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Package name:  awsevents\n",
      "Description:  minibar awsevents is a tiny utility that parses aws events like sns, s3, kinesis stream, dynamodb stream and returns a python object with no effort on your side .. image https travis ci.org canassa minibar.svg branch master alt travis ci build status target https travis ci.org canassa minibar installing .. code block python pip install awsevents\n",
      "Highest ranked keywords:\n",
      "\n",
      "              TF-IDF\n",
      "canassa     0.374170\n",
      "awsevents   0.374170\n",
      "minibar     0.374170\n",
      "travis      0.268772\n",
      "side        0.187085\n",
      "dynamodb    0.187085\n",
      "events      0.187085\n",
      "kinesis     0.187085\n",
      "stream      0.187085\n",
      "aws         0.187085\n",
      "parses      0.171673\n",
      "effort      0.171673\n",
      "tiny        0.171673\n",
      "utility     0.145326\n",
      "ci          0.145326\n",
      "returns     0.129915\n",
      "block       0.118980\n",
      "installing  0.118980\n",
      "alt         0.115937\n",
      "https       0.115610\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Package name:  baseq\n",
      "Description:  baseqs tools this is a tools for baseq applications... python packaging user guide packaging guide 's tutorial on packaging and distributing projects distribution tutorial . this project does not aim to cover best practices for python project development as a whole. for example, it does not provide guidance or tool recommendations for version control, documentation, or testing. the source for this project is available here src . most of the configuration for a python project is done in the setup.py file, an example of which is included in this project. you should edit this file accordingly to adapt this sample project to your needs. this is the readme file for the project. the file should use utf 8 encoding and can be written using restructuredtext rst or markdown md use with the appropriate key set md use . it will be used to generate the project webpage on pypi and will be displayed as the project homepage on common code hosting services, and should be written for that purpose. typical contents for this file would include an overview of the project, basic usage examples, etc. generally, including the project changelog in here is not a good idea, although a simple what's new section for the most recent version may be appropriate. packaging guide https packaging.python.org distribution tutorial https packaging.python.org en latest distributing.html src https github.com pypa sampleproject rst http docutils.sourceforge.net rst.html md https tools.ietf.org html rfc7764 section 3.5 commonmark variant md use https packaging.python.org specifications core metadata description content type optional\n",
      "Highest ranked keywords:\n",
      "\n",
      "                TF-IDF\n",
      "md            0.342452\n",
      "project       0.330757\n",
      "packaging     0.289896\n",
      "guide         0.240479\n",
      "tutorial      0.240479\n",
      "rst           0.171226\n",
      "file          0.158375\n",
      "section       0.151860\n",
      "distribution  0.151860\n",
      "https         0.144136\n",
      "written       0.139104\n",
      "use           0.135455\n",
      "src           0.129576\n",
      "tools         0.125582\n",
      "baseqs        0.093299\n",
      "guidance      0.093299\n",
      "aim           0.093299\n",
      "rfc7764       0.093299\n",
      "hosting       0.093299\n",
      "variant       0.093299\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Package name:  bethel-clustermgmt\n",
      "Description:  bethel.clustermgmt .. contents table of contents introduction this package contains support for managing and monitoring nodes in a cluster. when deploying changes to a zope cluster, it is necessary to proceed linearly across all nodes. each node should be taken out of service prior to any service disruption. load balancers typically use a configurable http health check, and if that health check fails enough times in a certain window, the node is taken out of service. this is how varnish works . before deploying changes, we simulate a service disruption on the node, causing the load balancer to take it out of service. this package contains a health status object which the load balancers call for health checks. we can inform the health status object that a node is to be taken out of service. it will then report the node as down returning an error for the health check , and the load balancer will take it out of service. because a load balancer may not send enough information to the backend zope node to enable it to effectively determine which node it is sounds odd, right , nodes need to be manually entered via the zmi manage screen. on the same screen, these nodes can be marked as offline. installation add bethel.clustermgmt to the eggs and zcml lists in the instance part of buildout.cfg, then rerun buildout. this package uses silva.core functionality to register itself with the zope infrastructure. as such it is listed as an extension in the silva extension service. it does not need activation in order to be used. a 'cluster health reporter' can now be found in the 'add' list in the zmi. configuration the management screen for a cluster health reporter has two sections. the first is the list of nodes, and the second provides an interface for taking nodes offline. list of nodes enter the list of nodes in the cluster, one per line. this does not need to be the fqdn of the node, but each node does need a unique entry. offline nodes the list of nodes is represented here with checkboxes. a node is out of offline out of service if it's box is checked. to manually change the service status of an node putting it online, taking it offline , check or uncheck the box for that node and click save offline nodes . use for monitoring the load balancer should be configured to query the health status object. if the zope node fails, the health status check will return a system error, or return no response at all hang . the load balancer will then automatically take the node out of service. upon recovery the health status checks will succeed, and the load balancer will automatically bring the node back into service. load balancer configuration varnish configuring varnish as a load balancer, and leveraging this health reporter is easy. let's assume the following 1. there are two nodes in your cluster, node1.example.com 8080 and node2.example.com 8080 2. the cluster health reporter is located at health add a director for these two nodes in the varnish vcl file director zope random .backend .host node1.example.com .port 8080 .first byte timeout 30s .weight 1 .backend .host node2.example.com .port 8080 .first byte timeout 30s .weight 1 a health check is called a probe in vcl. adding a probe to each backend, the vcl now looks like director silva23 random .backend .host node1.example.com .port 8080 .first byte timeout 30s .probe .url health node node1 .timeout 0.3 s .window 8 .threshold 3 .initial 3 .weight 1 .backend .host node2.example.com .port 8080 .first byte timeout 30s .probe .url health node node2 .timeout 0.3 s .window 8 .threshold 3 .initial 3 .weight 1 see the varnish configuration https www.varnish cache.org docs 3.0 for more information. use for deployments using a health status object, rather than an arbitrary web page, for the load balancers health check makes it useful for automatic service removal during system deployments. the node can me marked as 'out of service' via the zmi, or using rest. the rest approach is useful for automated deployment scripts. automated deployments rest api this object also responds to rest requests to adjust the service status. using this method, automated deployment scripts e.g. using fabric can take nodes out of service before deploying updates. access to the rest api calls are protected using the permission. to access the api calls, the request needs to be authenticated as a manager, or as a user in a role granting this permission. the rest api has two methods. 1 get the status of all nodes http get path to health rest nodestatus returns a json formatted dictionary of all nodes, and their status either online or offline , like this nodea status offline , nodeb status online 2 alter the status of one or more nodes http post path to health rest setstatus post data instructs the reporter on the new status for the given nodes. due to infrae.rest's lack of support for accepting json payloads, the json input is passed in via a post parameter named change . see the unittests for more info. the input format is the same the the output from rest nodestatus. use in fabric a simple python function can trigger a status change for a node. this in turn can be converted into a fabric task. the following is the fabric task we use at bethel for changing the service status of a node env.roledefs 'prod' 'node1.example.com', 'node2.example.com' , 'dev' 'test node.example.com' env.buildout root home zope silva23 buildout def alter service status newstatus alter the service status of a zope node, either putting online or offline host env 'host string' node host.split '.' 0 url 'http s 8080 silva varnish node is up rest setstatus' host query 'change' json.dumps node 'status' newstatus , 'skip bethel auth' 1 req urllib2.request url, query authh basic base64.encodestring ' s s' rest creds 1 req.add header authorization , authh response urllib2.urlopen req, urllib.urlencode query back ''.join response.readlines return 'ok' the username and password are read from a protected file when the fabfile is loaded. this task in turn can be used as a component of a larger automated deployment task this is the rest of of bethel's fabfile def buildout with prefix export home home zope with cd env.buildout root sudo hg debug pull u env, user zope sudo . bin buildout env, user zope def restart apache using the sudo command does not work it issues the following sudo s p 'sudo password ' bin bash l c etc init.d httpd restart which runs a shell executing the command in quotes. ross was not able to configure sudo to allow multiple httpd options with one line, but suggested the run command instead. sudo etc init.d httpd restart run sudo etc init.d httpd restart def push buildout apache restart true if type apache restart in stringtypes apache restart apache restart 'true' change status false if env.host string in env.roledefs 'prod' change status true take out of service, it takes less time to take out of service than it does to put back into service if change status puts taking offline sleeping 20 seconds alter service status 'offline' sleep 20 buildout if apache restart restart apache todo test some urls, loading up the local zodb cache before bringing back in to service put back into service if change status puts taking online sleeping 30 seconds alter service status 'online' sleep 30 adding fabric to your buildout is detailed here http www.vlent.nl weblog 2010 09 27 fabric easy deployment this fabfile is located in the buildout root. running an automated deployment of our production environment is simple . bin fab r prod push buildout when using mod wsgi to serve zope, a restart of apache is required for change to take effect. if for any reason you'd want to push buildout but not restart apache, pass in false to the restart apache per task argument . bin fab r prod push buildout restart apache false the combination of fabric and bethel.clustermgmt has decreased deployment time considerably. it is now one command run in the background, whereas before it was a 5 10 minute long repetitive rinse repeat cycle for each node in the cluster. bethel.clustermgmt changlog bethel.clustermgmt 1.0 2012 04 30 first release of cluster mgmt utilities\n",
      "Highest ranked keywords:\n",
      "\n",
      "              TF-IDF\n",
      "node        0.404461\n",
      "health      0.329165\n",
      "nodes       0.269641\n",
      "restart     0.247428\n",
      "service     0.235201\n",
      "status      0.219338\n",
      "rest        0.182025\n",
      "buildout    0.165477\n",
      "offline     0.154080\n",
      "load        0.152189\n",
      "apache      0.149611\n",
      "zope        0.143579\n",
      "fabric      0.134820\n",
      "balancer    0.134820\n",
      "8080        0.123714\n",
      "varnish     0.115560\n",
      "change      0.106996\n",
      "deployment  0.106041\n",
      "sudo        0.104728\n",
      "take        0.100505\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Package name:  bundlewrap\n",
      "Description:  by allowing for easy and low overhead config management, bundlewrap fills the gap between complex deployments using chef or puppet and old school system administration over ssh. while most other config management systems rely on a client server architecture, bundlewrap works off a repository cloned to your local machine. it then automates the process of sshing into your servers and making sure everything is configured the way it's supposed to be. you won't have to install anything on managed servers.\n",
      "Highest ranked keywords:\n",
      "\n",
      "                  TF-IDF\n",
      "bundlewrap      0.357740\n",
      "config          0.266686\n",
      "low             0.178870\n",
      "overhead        0.178870\n",
      "chef            0.178870\n",
      "fills           0.178870\n",
      "puppet          0.178870\n",
      "automates       0.178870\n",
      "supposed        0.178870\n",
      "school          0.178870\n",
      "gap             0.178870\n",
      "sshing          0.178870\n",
      "administration  0.164135\n",
      "managed         0.164135\n",
      "old             0.164135\n",
      "systems         0.153680\n",
      "configured      0.153680\n",
      "rely            0.153680\n",
      "deployments     0.153680\n",
      "cloned          0.153680\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Package name:  charade\n",
      "Description:  charade the universal character encoding detector   detects ascii, utf 8, utf 16 2 variants , utf 32 4 variants  big5, gb2312, euc tw, hz gb 2312, iso 2022 cn traditional and simplified chinese  euc jp, shift jis, iso 2022 jp japanese  euc kr, iso 2022 kr korean  koi8 r, maccyrillic, ibm855, ibm866, iso 8859 5, windows 1251 cyrillic  iso 8859 2, windows 1250 hungarian  iso 8859 5, windows 1251 bulgarian  windows 1252 english  iso 8859 7, windows 1253 greek  iso 8859 8, windows 1255 visual and logical hebrew  tis 620 thai   requires python 2.6 or later  command line tool   chardet comes with a command line script which reports on the encodings of one or more files   charade somefile someotherfile somefile windows 1252 with confidence 0.5 someotherfile ascii with confidence 1.0  about   this is a port of mark pilgrim's excellent chardet. previous two versions  needed to be maintained one that supported python 2.x and one that supported  python 3.x. with the minor amount of work placed into this port, charade now  supports both in one codebase.  the base for the work was mark's last available copy of the chardet source for  python 3000.  the reason   does everything have to have a reason no, but in this case the reason was to  help out requests http python requests.org and anyone else who sorely  needed this.  what about x, y, or z    if x, y, or z a colloquialism for other projects that may do the same thing  do exist and indeed existed before charade, then i'm disappointed that they  didn't make themselves better known. it would have saved me quite a few hours.   maintainer ian cordasco   1.0.3 2012 01 17   rename chardet.py script to charade\n",
      "Highest ranked keywords:\n",
      "\n",
      "                 TF-IDF\n",
      "iso            0.434671\n",
      "windows        0.330006\n",
      "8859           0.316199\n",
      "charade        0.252959\n",
      "2022           0.189719\n",
      "euc            0.189719\n",
      "utf            0.154401\n",
      "reason         0.147373\n",
      "variants       0.126480\n",
      "someotherfile  0.126480\n",
      "somefile       0.126480\n",
      "confidence     0.126480\n",
      "chardet        0.126480\n",
      "1252           0.126480\n",
      "1251           0.126480\n",
      "one            0.115524\n",
      "needed         0.098249\n",
      "python         0.089657\n",
      "supported      0.080437\n",
      "work           0.074703\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Package name:  collective-jsonify\n",
      "Description:  collective.jsonify exports your plone content to json . many packages that export data from plone have complicated dependencies, and so only work with plone 3.0 or higher or not even with 3.0 . collective.jsonify 's only dependency is simplejson . it can be installed in any plone version as far back as plone 2.1 or probably even plone 2.0, but not tested zope 2.6.4 with cmf rather than plone python 2.2 the exported json is a friendly format. install collective.jsonify on a site you want to export from, and setup an import transmogrifier pipeline on the site you're importing to, using the blueprints in the collective.jsonmigrator package. alternatively use the provided export script by adding it to an external method as described in using the exporter . for more information see the documentation . warning this product may contain traces of nuts. author rok garbas , migrating for you since 2008 source http github.com collective collective.jsonify .. simplejson http pypi.python.org simplejson .. json http en.wikipedia.org wiki json .. http pypi.python.org pypi .. collective.jsonmigrator http pypi.python.org pypi collective.jsonmigrator .. using the exporter https en latest using the exporter .. documentation https .. rok garbas http www.garbas.si labs plone migration changelog 1.4 2018 09 20 try subject and contributors when subject and contributors give nothing. maurits check if value exists on file like fields agitator export creation date and modification date for all objects, not only is cmf only obj. for dexterity these values are not present in a schemata, so they are not included as part of the normal schemata based dexterity export. sunew use self.decode method to get stringified field value when wrapping content for export. instification 1.3 2017 12 21 export plone.app.redirector redirects, if available. comply with default expectations of redirector section in hvelarde do not export formgen and redirection tools. hvelarde show translations from linguaplone if canonical is available. agitator fixed value for unknown fields. the value was never calculated fresh for these fields, so you got the value of the previous field. or you probably got a nameerror if this was the first field. maurits fix manifest added changes.rst merge union to .gitattributes ale rt added the history to json export. rristow 1.2 2016 05 24 do not require simplejson if we already have the native json module ale rt when doing an export with export content and having constraints to skip items, still allow to walk into subitems of the skipped ones except for skipped paths, where the whole path is skipped. thet 1.1 2015 10 22 set json repsonse headers jensens 1.0 2015 05 16 let the wrapper test correctly for zope.interface and interface interfaces. thet in the wrapper class, call the value in decode, if it's a callable. thet when serializing datetime, date, time or datetime properties, just use the unicode representation which can be parsed. thet when serializing values, if there is no special handler for a field type, just try to unicode the value. thet fix export of defaultpage and layout. before, always the defaultpage was set now layout is always set and defaultpage only, if there is one defined. thet handle dexterity field types. thet check, if wrapper methods for zope cmf objects are zope cmf only objects by testing for archetypes and dexterity first. thet add blobfield for get archetypes fields . thet don't try to convert ints to unicode in get properties . djowett zope 2.6 support for collective.jsonify. djowett fix setup.py to work with python 2.2. djowett add error type to tracebacks. djowett fix read of namedblobimage, namedfile and namedblobfile in dexterity objects. djowett fix read of field for unicode transcoding in dexterity objects. djowett make support more generic and handle probably most use cases. thet add directly provided export field for the object's directly provided interfaces. thet add json methods module to own extension folder, which makes it automatically available and unnecessary to add it to the instance's extension folder. thet don't skip computedfield fields, but just export their computed value. better skip them in your transmogrifier import pipeline. thet allow a skip callback function to be passed to the export content function. it evaluates to true , if the current visited item should be excluded from exporting. thet export a content's references as list of uid values. thet declare the content type of a field's value only for textfield and stringfield . thet add example buildouts for plone 2.1, 2.5, 3 and 4. thet declare base64 encoding for datafield fieldname structures. this is used to correctly decode in transmogrify.dexterity. thet add export module from and modify to use collective.jsonify wrapper. use it in plone 2.1 by adding it as external method. thet pep 8. thet fixing local roles export. realefab make atextensionfields serializable. jsbueno fixes exporting of image types that use atblob. jsbueno 0.2 2014 08 18 support p.a.collection queryfield. jone dexterity support. djowett add blob fields support. use specific methods to retrieve filename, content type and size. gborelli add get at field value to wrappe.wrapper in order to use accessor method for archetypes fields. gborelli jsonify view added. see readme jsonify view.rst for more pieretti 0.1 2011 03 14 documentation added garbas collection of external methods from and collective.sync migrator . garbas initial release garbas\n",
      "Highest ranked keywords:\n",
      "\n",
      "               TF-IDF\n",
      "thet         0.573647\n",
      "export       0.311924\n",
      "plone        0.246431\n",
      "djowett      0.200776\n",
      "dexterity    0.200776\n",
      "json         0.154428\n",
      "garbas       0.143412\n",
      "add          0.132379\n",
      "value        0.124505\n",
      "field        0.123623\n",
      "content      0.115821\n",
      "simplejson   0.114729\n",
      "cmf          0.114729\n",
      "fix          0.093740\n",
      "use          0.093695\n",
      "skip         0.093371\n",
      "unicode      0.089121\n",
      "exporter     0.086047\n",
      "archetypes   0.086047\n",
      "defaultpage  0.086047\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "packages = list(preprocessed_projects_with_keywords_dataset_ordered.keys())[:10]\n",
    "package_indexes = OrderedDict({package : index for index, package in enumerate(packages)})\n",
    "\n",
    "for package_name in packages:\n",
    "    df_keywords = pd.DataFrame(tf_idf_keywords[package_indexes[package_name]].T.todense(), index=vectorizer.get_feature_names_out(), columns=[\"TF-IDF\"])\n",
    "    df_keywords = df_keywords.sort_values(\"TF-IDF\", ascending=False)\n",
    "\n",
    "    print(\"Package name: \", package_name)\n",
    "    print(\"Description: \", projects_with_keywords_dataset[package_name][0])\n",
    "    print(\"Highest ranked keywords:\\n\")\n",
    "    print(df_keywords.head(20))\n",
    "\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, we can see that the highest ranked tokens seem to be relevant descriptors of the projects, with the exception of words that could be used to describe any type of package (\"pypi\", \"python\", \"module\"...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting keywords with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we managed to rank the most important tokens of a project description using TF-IDF, we could try to see if the highest ranked tokens for each package are effective keywords predictors. (WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting packages trove classifiers with Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See: https://arxiv.org/pdf/1908.10419.pdf"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
